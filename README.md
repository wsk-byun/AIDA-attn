# AIDA-attn

We implemented the attention blocks based on [CyberZHG](https://github.com/CyberZHG/keras-self-attention) and [deepak-kaji](https://github.com/deepak-kaji/mimic-lstm/).

View presentation slides: https://wsk-byun.github.io/AIDA-attn/
